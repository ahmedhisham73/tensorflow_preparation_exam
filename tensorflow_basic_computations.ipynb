{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_basic_computations.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIv6BtEDixO7"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv7YzUDwi8HK",
        "outputId": "71bba3d0-2a13-4b77-be4e-acbd65c57b1e"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhKCxX0gjLMx",
        "outputId": "14c68209-6c15-4776-88b4-106eb3422762"
      },
      "source": [
        "# Create an empty graph, that will be filled with operation nodes later\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "\n",
        "# Register this graph as default graph. \n",
        "# All operations within this context will become operation nodes in this graph\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Create two constants of value 5\n",
        "    a=tf.constant(5.0)\n",
        "    b=tf.constant(5.0)\n",
        "    \n",
        "    # Multiply the constants with each other\n",
        "    c=tf.multiply(a,b)\n",
        "    \n",
        "\n",
        "# Create a session to execute the dataflow graph\n",
        "with tf.Session(graph=main_graph) as session:\n",
        "    \n",
        "    # Perform the calculation defined in the dataflow graph and get the result\n",
        "    output=session.run(c)\n",
        "    print('Result of the multiplication: %d '%output)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result of the multiplication: 25 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJFZ8Qq5jN-i",
        "outputId": "c205266c-640e-4f52-ab9f-31f9ee0db28e"
      },
      "source": [
        "# Create an empty graph, that will be filled with operation nodes later\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "# Register this graph as default graph. \n",
        "# All operations within this context will become operation nodes in this graph\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Define the placeholders that will feed python arrays into the dataflow graph\n",
        "    a=tf.placeholder(name='a', shape=[5], dtype=tf.float32)\n",
        "    b=tf.placeholder(name='b', shape=[5], dtype=tf.float32)\n",
        "    \n",
        "    c=tf.multiply(a,b)\n",
        "    \n",
        "# Create a session to execute the dataflow graph\n",
        "with tf.Session(graph=main_graph) as session:\n",
        "    \n",
        "    # Perform the calculation defined in the dataflow graph and get the result.\n",
        "    # We must provide the values for the placeholders with \"feed_dict\" dictionary\n",
        "    output=session.run(c, feed_dict={a: [5.0,7.0,3.0,9.0,2.0],\n",
        "                                     b: [1.0,2.0,4.0,8.0,4.0],\n",
        "                                     })\n",
        "    print(output)\n",
        "\n",
        "   "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 5. 14. 12. 72.  8.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPfKL13FjXLg",
        "outputId": "cad2272e-e38c-4944-efe9-80cc0f1e4ab9"
      },
      "source": [
        "# Import numpy for some array reshaping\n",
        "import numpy as np\n",
        "\n",
        "# Create an empty graph, that will be filled with operation nodes later\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "# Register this graph as default graph. \n",
        "# All operations within this context will become operation nodes in this graph\n",
        "with main_graph.as_default():\n",
        "    # Placeholder for data input\n",
        "    input_=tf.placeholder(dtype=tf.float32, shape=[1,5], name='input')\n",
        "    \n",
        "    ###Define the weight matrices###\n",
        "    # Weight matrix, that connects the input and the 1st hidden layer\n",
        "    W1=tf.get_variable(name='W1', shape=[5,10], initializer=tf.random_normal_initializer)\n",
        "    # Weight matrix, that connects the 1st hidden layer and the 2nd hidden layer\n",
        "    W2=tf.get_variable(name='W2', shape=[10,10], initializer=tf.random_normal_initializer)\n",
        "    # Weight matrix, that connects the 2nd hidden layer and the 3rd hidden layer\n",
        "    W3=tf.get_variable(name='W3', shape=[10,10], initializer=tf.random_normal_initializer)\n",
        "    # Weight matrix, that connects the 3rd hidden layer and the output layer\n",
        "    W4=tf.get_variable(name='W4', shape=[10,3], initializer=tf.random_normal_initializer)\n",
        "         \n",
        "    ####Define the forward propagation operations###\n",
        "      \n",
        "    #1st hidden layer\n",
        "    z1=tf.matmul(input_, W1)\n",
        "    a1=tf.nn.tanh(z1)\n",
        "    \n",
        "    #2nd hidden layer\n",
        "    z2=tf.matmul(a1, W2)\n",
        "    a2=tf.nn.tanh(z2)\n",
        "    \n",
        "    #3rd hidden layer\n",
        "    z3=tf.matmul(a2, W3)\n",
        "    a3=tf.nn.tanh(z3)\n",
        "    \n",
        "    #output layer\n",
        "    z_out=tf.matmul(a3, W4)\n",
        "    output=tf.nn.tanh(z_out)\n",
        "\n",
        "\n",
        "# Create a session to execute the dataflow graph   \n",
        "with tf.Session(graph=main_graph) as sess:\n",
        "    \n",
        "    # Initialize the weight matrix\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Define some random input\n",
        "    x=np.array([1.0, 2.5, 0.7, 3.0, 9.0]).reshape([1,5])\n",
        "    \n",
        "    # Start the forward propagation step\n",
        "    prediction=sess.run(output, feed_dict={input_: x})\n",
        "    \n",
        "    print('\\n\\n\\nResult of the forward propagation: \\n')\n",
        "    print(prediction)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Result of the forward propagation: \n",
            "\n",
            "[[0.9986295 0.1366151 0.9927698]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPmetYAxjfhP",
        "outputId": "04499ecb-fa95-4fd3-8fca-57fbc7a7f988"
      },
      "source": [
        "# Import numpy for some array reshaping\n",
        "import numpy as np\n",
        "\n",
        "# Create an empty graph, that will be filled with operation nodes later\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "# Register this graph as default graph. \n",
        "# All operations within this context will become operation nodes in this graph\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Placeholder for the input data\n",
        "    input_= tf.placeholder(dtype=tf.float32, shape=[1,3], name='input')\n",
        "    # Placeholder for the label\n",
        "    label = tf.placeholder(dtype=tf.float32, shape=[1,2], name='label')\n",
        "    \n",
        "    # Define a 3x2 weight matrix\n",
        "    W=tf.get_variable(name='weights', shape=[3,2], dtype=tf.float32)\n",
        "    \n",
        "    # Compute a forward propagation step\n",
        "    output=tf.nn.tanh(tf.matmul(input_, W))\n",
        "    \n",
        "    # Use build-in function for the mean squared error loss\n",
        "    loss_op=tf.losses.mean_squared_error(labels=label, predictions=output)\n",
        "    \n",
        "\n",
        "# Create a session to execute the dataflow graph\n",
        "\n",
        "with tf.Session(graph=main_graph) as sess:\n",
        "    \n",
        "    # Initialize the weight matrix\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Define some random input (x) and a label (y)\n",
        "    # Reshape the arrays into the shape of the placeholders\n",
        "    x=np.reshape([10.0,3.0,4.0], [1,3])\n",
        "    y=np.reshape([5.0,2.0], [1,2])\n",
        "\n",
        "    # Compute the prediction of the network\n",
        "    nn_output=sess.run(output, feed_dict={input_: x})\n",
        "\n",
        "    # Compute the mean squared error loss\n",
        "    mse_loss=sess.run(loss_op, feed_dict={input_: x,\n",
        "                                         label: y})\n",
        "    print('\\n\\n\\nOutput of the neural network: \\n')\n",
        "    print(nn_output)\n",
        "    \n",
        "    print('\\nMean squared error loss: %.2f'%mse_loss)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Output of the neural network: \n",
            "\n",
            "[[ 0.99999946 -1.        ]]\n",
            "\n",
            "Mean squared error loss: 12.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lOW9GRdjwjO",
        "outputId": "28225dd4-8ebb-4d4e-91fc-b800ab15ba4c"
      },
      "source": [
        "# Import numpy for some array reshaping\n",
        "import numpy as np\n",
        "\n",
        "# Create an empty graph, that will be filled with operation nodes later\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "# Register this graph as default graph. \n",
        "# All operations within this context will become operation nodes in this graph\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Placeholder for the input data\n",
        "    input_= tf.placeholder(dtype=tf.float32, shape=[1,3], name='input')\n",
        "    # Placeholder for the scalar label\n",
        "    label = tf.placeholder(dtype=tf.int32, shape=[1], name='label_sparse')\n",
        "    # Placeholder for the one-hot-encoded label\n",
        "    label_ohe = tf.placeholder(dtype=tf.float32, shape=[1,5], name='label_one_hot_encoded')\n",
        "\n",
        "    # Define a 3x5 weight matrix\n",
        "    W=tf.get_variable(name='weights', shape=[3,5], dtype=tf.float32)\n",
        "    \n",
        "    # Forward propagation without an activation function\n",
        "    logits=tf.matmul(input_, W)\n",
        "    \n",
        "    # Cross entropy loss operation, that requires the one-hot-encoded label\n",
        "    loss_op=tf.nn.softmax_cross_entropy_with_logits_v2(labels=label_ohe, \n",
        "                                                       logits=logits, \n",
        "                                                       name='cross_entropy_loss')\n",
        "\n",
        "    # Cross entropy loss operation, that requires the scalar label\n",
        "    loss_op_sparse=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, \n",
        "                                                                  logits=logits, \n",
        "                                                                  name='sparse_cross_entropy_loss')\n",
        "\n",
        "\n",
        "# Create a session to execute the dataflow graph\n",
        "with tf.Session(graph=main_graph) as sess:\n",
        "    \n",
        "    # Initialize the weight matrix\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Define some random input (x)\n",
        "    x=np.reshape([10.0,3.0,4.0], [1,3])\n",
        "    \n",
        "    # Define the scalar label \n",
        "    y=np.reshape([2], [1])\n",
        "    # The same label but one-hot-encoded version of it\n",
        "    y_ohe=np.reshape([0,0,1,0,0], [1,5])\n",
        "    \n",
        "    # Run the one cross entropy loss operation\n",
        "    loss=sess.run(loss_op, feed_dict={input_:x, \n",
        "                                      label_ohe: y_ohe\n",
        "                                      })\n",
        "    \n",
        "    # Run the other cross entropy loss operation\n",
        "    sparse_loss=sess.run(loss_op_sparse, feed_dict={input_:x,\n",
        "                                                    label: y\n",
        "                                                    })\n",
        "    \n",
        "    print('\\n\\n\\nCross entropy loss: %.2f'%loss)\n",
        "    print('Sparse cross entropy loss: %.2f'%sparse_loss)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Cross entropy loss: 0.18\n",
            "Sparse cross entropy loss: 0.18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbusiu2Lj1o2"
      },
      "source": [
        "# Import numpy for some array reshaping\n",
        "import numpy as np\n",
        "\n",
        "# Create an empty graph, that will be filled with operation nodes later\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "# Register this graph as default graph. \n",
        "# All operations within this context will become operation nodes in this graph\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Placeholder for data input\n",
        "    input_=tf.placeholder(dtype=tf.float32, shape=[1,5], name='input')\n",
        "    # Placeholder for the label\n",
        "    labels=tf.placeholder(dtype=tf.float32, shape=[1,1], name='labels')\n",
        "\n",
        "    # Define a 5x1 weight matrix\n",
        "    W=tf.get_variable(name='weights', shape=[5,1])\n",
        "    \n",
        "    # Forward propagation\n",
        "    forward=tf.nn.tanh(tf.matmul(input_, W))\n",
        "\n",
        "    # Mean squared error loss function\n",
        "    loss=tf.losses.mean_squared_error(labels, forward)\n",
        "    \n",
        "    # Define the instance of the class that performs the gradient descent step\n",
        "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "    \n",
        "    # Get all trainable parameters of the network (here: the weight matrix W)\n",
        "    trainable_variables=tf.trainable_variables()\n",
        "    \n",
        "    # Compute the gradients of the loss function with respect to the weights\n",
        "    gradients= tf.gradients(loss, trainable_variables)\n",
        "    \n",
        "    # Perform the gradient descent step.\n",
        "    # The input argument are tuples. Each tuple is a pair of the gradient and the weight \n",
        "    # that was used to calculate this gradient\n",
        "    update_step=optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "     \n",
        "    \n",
        "    #Alternative Solution \n",
        "    update_step_alternative=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
        "       \n",
        "    \n",
        "    \n",
        "# Create a session to execute the dataflow graph   \n",
        "with tf.Session(graph=main_graph) as sess:\n",
        "    \n",
        "    # Initialize the weight matrix\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # Define some random input and a label\n",
        "    x=np.array([1,1,0,2,5]).reshape([1,5])\n",
        "    y=np.array([2]).reshape([1,1]) \n",
        "    \n",
        "    # Perform one single gradient descent step\n",
        "    sess.run(update_step, feed_dict={input_: x,\n",
        "                                     labels: y\n",
        "                                     }) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8VmkHjYj_VG",
        "outputId": "5a2da85e-39fa-4993-8b11-5a7e3906292b"
      },
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "# list that will contain our training set\n",
        "training_data=[]\n",
        "\n",
        "# How many digits should this binary number have?\n",
        "n=11\n",
        "    \n",
        "#Mini-batch size\n",
        "batch_size=16\n",
        "\n",
        "print('\\n\\nGeneration of Data...\\n') \n",
        "for i in np.arange(2048):\n",
        "    \n",
        "    # Create a binary number of type string\n",
        "    b = bin(i)[2:]\n",
        "    l = len(b)\n",
        "    b = str(0) * (n - l) + b  \n",
        "\n",
        "    # Convert the binary string number to type float\n",
        "    features=np.array(list(b)).astype(float)\n",
        "    \n",
        "    # Create and normalize the corresponding decimal label\n",
        "    label=float(i)/2047\n",
        "    \n",
        "    # Put the feature-label instance into the list\n",
        "    training_data.append([features, label])\n",
        "    \n",
        "    if (i>=1 and i<11) or (i>=2038):\n",
        "        print('binary number: %s, decimal number: %d' %(b, i))\n",
        "        \n",
        "# shuffle the data\n",
        "shuffle(training_data)  \n",
        "\n",
        "# convert the list to np.array     \n",
        "training_data=np.array(training_data)\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "# Get the next mini-batch of training samples\n",
        "def get_next_batch(n_batch):\n",
        "    \n",
        "    # Get the next mini-batch of training samples from the dataset\n",
        "    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n",
        "    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n",
        "    \n",
        "    # Reshape the list of arrays into a nxn np.array\n",
        "    features = np.concatenate(features).reshape([batch_size,11])  \n",
        "    # Reshape the labels \n",
        "    labels=np.reshape(labels, [batch_size,1])\n",
        "    \n",
        "    return features, labels\n",
        "    \n",
        "features, labels=get_next_batch(n_batch=1)\n",
        "\n",
        "print(features)\n",
        "print(labels)\n",
        "#%%  \n",
        "\n",
        "# Create the training graph\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Define the placeholders for the features and the labels\n",
        "    x=tf.placeholder(dtype=tf.float32,shape=[batch_size,11], name='features')\n",
        "    y=tf.placeholder(dtype=tf.float32, shape=[batch_size,1], name='labels')\n",
        "           \n",
        "    # Create the weight matrices and the bias vectors \n",
        "    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n",
        "   \n",
        "    W1=tf.get_variable('W1',shape=[11,50], initializer=initializer)\n",
        "    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n",
        "    W3=tf.get_variable('W3',shape=[25,1], initializer=initializer)\n",
        "    \n",
        "    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n",
        "    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n",
        "\n",
        "    ### Define the forward propagation step ###\n",
        "    \n",
        "    # First hidden layer\n",
        "    z1=tf.matmul(x,W1)+b1\n",
        "    a1=tf.nn.tanh(z1)\n",
        "    \n",
        "    # Second hidden layer\n",
        "    z2=tf.matmul(a1,W2)+b2\n",
        "    a2=tf.nn.tanh(z2)\n",
        "    \n",
        "    # Outputlayer\n",
        "    predict_op=tf.nn.relu(tf.matmul(a2,W3))\n",
        "    \n",
        "    # Define the loss function\n",
        "    loss_op=tf.losses.mean_squared_error(y,predict_op)\n",
        "       \n",
        "    # Perform a gradient descent step\n",
        "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "    trainable_parameters = tf.trainable_variables()\n",
        "    gradients = tf.gradients(loss_op,trainable_parameters)\n",
        "    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n",
        "\n",
        "\n",
        "print('\\n\\nStart of the training...\\n')\n",
        "with tf.Session(graph=main_graph) as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # How many mini-batches in total?\n",
        "    num_batches=int(2048/batch_size)\n",
        "    \n",
        "    loss=0\n",
        "\n",
        "    #Iterate over the entire training set for 10 times\n",
        "    for epoch in range(10):\n",
        "            \n",
        "        # Iterate over the number of mini-batches\n",
        "        for n_batch in range(num_batches-1):\n",
        "            \n",
        "            # Get the next mini-batches of samples for the training set\n",
        "            features, labels=get_next_batch(n_batch)\n",
        "              \n",
        "            # Perform the gradient descent step on that mini-batch and compute the loss value\n",
        "            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n",
        "            loss+=loss_\n",
        "             \n",
        "        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss/num_batches)))\n",
        "        loss=0 \n",
        "\n",
        "    # Compute the prediction on the last mini-batch\n",
        "    prediction=sess.run(predict_op, feed_dict={x:features})\n",
        "    \n",
        "    # Iterate over the features and labels from the last mini-batch as well as\n",
        "    # the predicitons made by the network, and compare them to check the performance\n",
        "    for f, l, p in zip(features, labels, prediction):\n",
        "        \n",
        "        # Rescale the predictions and labels back into their original value range\n",
        "        p=p*2047\n",
        "        l=l*2047\n",
        "        \n",
        "        print('Binary number: %s,  label: %i, prediciton %i' %(str(f), l,p))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Generation of Data...\n",
            "\n",
            "binary number: 00000000001, decimal number: 1\n",
            "binary number: 00000000010, decimal number: 2\n",
            "binary number: 00000000011, decimal number: 3\n",
            "binary number: 00000000100, decimal number: 4\n",
            "binary number: 00000000101, decimal number: 5\n",
            "binary number: 00000000110, decimal number: 6\n",
            "binary number: 00000000111, decimal number: 7\n",
            "binary number: 00000001000, decimal number: 8\n",
            "binary number: 00000001001, decimal number: 9\n",
            "binary number: 00000001010, decimal number: 10\n",
            "binary number: 11111110110, decimal number: 2038\n",
            "binary number: 11111110111, decimal number: 2039\n",
            "binary number: 11111111000, decimal number: 2040\n",
            "binary number: 11111111001, decimal number: 2041\n",
            "binary number: 11111111010, decimal number: 2042\n",
            "binary number: 11111111011, decimal number: 2043\n",
            "binary number: 11111111100, decimal number: 2044\n",
            "binary number: 11111111101, decimal number: 2045\n",
            "binary number: 11111111110, decimal number: 2046\n",
            "binary number: 11111111111, decimal number: 2047\n",
            "[[0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0.]\n",
            " [0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.]\n",
            " [0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
            " [1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1.]\n",
            " [0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0.]\n",
            " [1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 1.]\n",
            " [1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0.]\n",
            " [1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.]\n",
            " [1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0.]\n",
            " [0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0.]\n",
            " [0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.]]\n",
            "[[0.2432828529555447]\n",
            " [0.4987787005373718]\n",
            " [0.24914509037616023]\n",
            " [0.7274059599413776]\n",
            " [0.285295554469956]\n",
            " [0.9130434782608695]\n",
            " [0.10454323400097704]\n",
            " [0.8842208109428432]\n",
            " [0.605276013678554]\n",
            " [0.8446507083536883]\n",
            " [0.9653150952613581]\n",
            " [0.6946751343429409]\n",
            " [0.0810942843185149]\n",
            " [0.9506595017098193]\n",
            " [0.4582315583781143]\n",
            " [0.22618466047874938]]\n",
            "\n",
            "\n",
            "Start of the training...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch_nr.: 0, loss: 0.223\n",
            "epoch_nr.: 1, loss: 0.036\n",
            "epoch_nr.: 2, loss: 0.012\n",
            "epoch_nr.: 3, loss: 0.009\n",
            "epoch_nr.: 4, loss: 0.007\n",
            "epoch_nr.: 5, loss: 0.007\n",
            "epoch_nr.: 6, loss: 0.006\n",
            "epoch_nr.: 7, loss: 0.006\n",
            "epoch_nr.: 8, loss: 0.005\n",
            "epoch_nr.: 9, loss: 0.005\n",
            "Binary number: [0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0.],  label: 268, prediciton 84\n",
            "Binary number: [1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.],  label: 1509, prediciton 1672\n",
            "Binary number: [0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1.],  label: 97, prediciton 0\n",
            "Binary number: [1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.],  label: 1807, prediciton 1903\n",
            "Binary number: [0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0.],  label: 198, prediciton 176\n",
            "Binary number: [0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0.],  label: 726, prediciton 519\n",
            "Binary number: [0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1.],  label: 77, prediciton 0\n",
            "Binary number: [1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0.],  label: 1630, prediciton 1608\n",
            "Binary number: [0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1.],  label: 509, prediciton 615\n",
            "Binary number: [1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0.],  label: 1374, prediciton 1543\n",
            "Binary number: [0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1.],  label: 817, prediciton 655\n",
            "Binary number: [0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0.],  label: 350, prediciton 428\n",
            "Binary number: [1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1.],  label: 1191, prediciton 1084\n",
            "Binary number: [0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.],  label: 524, prediciton 535\n",
            "Binary number: [0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1.],  label: 745, prediciton 723\n",
            "Binary number: [1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.],  label: 1386, prediciton 1392\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAQjRptSkHcL",
        "outputId": "4a78c564-f885-4f62-eaf3-5171080865fd"
      },
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "# list that will contain our training set\n",
        "training_data=[]\n",
        "\n",
        "# How many digits should this binary number have?\n",
        "n=4\n",
        "    \n",
        "#Mini-batch size\n",
        "batch_size=8\n",
        "\n",
        "print('\\n\\nGeneration of Data...\\n') \n",
        "for i in np.arange(0, 10):\n",
        "    \n",
        "    # Create a binary number of type string\n",
        "    b = bin(i)[2:]\n",
        "    l = len(b)\n",
        "    b = str(0) * (n - l) + b  \n",
        "\n",
        "    # Convert binary string number to type float\n",
        "    features=np.array(list(b)).astype(float)\n",
        "    # Create the corresponding binary label / class\n",
        "    label=i\n",
        "    \n",
        "    # Put the feature-label pair into the list\n",
        "    training_data.append([features, label])\n",
        "\n",
        "    print('binary number: %s, decimal number: %d' %(b, i))\n",
        "        \n",
        "    \n",
        "#%%\n",
        "# shuffle the data\n",
        "shuffle(training_data)  \n",
        "\n",
        "training_data=training_data*1000\n",
        "\n",
        "# convert the list to np.array     \n",
        "training_data=np.array(training_data)\n",
        "\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "# Get the next mini-batch of training samples\n",
        "def get_next_batch(n_batch):\n",
        "    \n",
        "    # Get the next mini-batch of training samples from the dataset\n",
        "    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n",
        "    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n",
        "    \n",
        "    # Reshape the list of arrays into a nxn np.array\n",
        "    features = np.concatenate(features).reshape([batch_size, 4])  \n",
        "    # Reshape the labels \n",
        "    labels=np.reshape(labels, [batch_size])\n",
        "    \n",
        "    return features, labels\n",
        "    \n",
        "features, labels=get_next_batch(n_batch=1)\n",
        "\n",
        "print('\\n\\nMini-batch of features: \\n')\n",
        "print(features)\n",
        "print('\\n\\nMini-batch of labels: \\n')\n",
        "print(labels)\n",
        "\n",
        "#%%  \n",
        "\n",
        "# Create the training graph\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Define the placeholders for the features and the labels\n",
        "    x=tf.placeholder(dtype=tf.float32,shape=[batch_size, 4], name='features')\n",
        "    y=tf.placeholder(dtype=tf.int32, shape=[batch_size], name='labels')\n",
        "           \n",
        "    # Create the weight matrices and the bias vectors \n",
        "    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n",
        "   \n",
        "    W1=tf.get_variable('W1',shape=[4,50], initializer=initializer)\n",
        "    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n",
        "    W3=tf.get_variable('W3',shape=[25,10], initializer=initializer)\n",
        "    \n",
        "    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n",
        "    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n",
        "\n",
        "    ### Define the forward propagation step ###\n",
        "    \n",
        "    # First hidden layer\n",
        "    z1=tf.matmul(x,W1)+b1\n",
        "    a1=tf.nn.tanh(z1)\n",
        "    \n",
        "    # Second hidden layer\n",
        "    z2=tf.matmul(a1,W2)+b2\n",
        "    a2=tf.nn.tanh(z2)\n",
        "    \n",
        "    # Outputlayer, without an activation function (input for the loss function)\n",
        "    logits=tf.matmul(a2,W3)\n",
        "       \n",
        "    # Compute the probability scores after the training)\n",
        "    probs=tf.nn.softmax(logits)\n",
        "    \n",
        "    # Define the loss function\n",
        "    loss_op=tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
        "       \n",
        "    # Perform a gradient descent step\n",
        "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
        "    trainable_parameters = tf.trainable_variables()\n",
        "    gradients = tf.gradients(loss_op,trainable_parameters)\n",
        "    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n",
        "\n",
        "\n",
        "print('\\n\\nStart of the training...\\n')\n",
        "with tf.Session(graph=main_graph) as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # How many mini-batches in total?\n",
        "    num_batches=int(10000/batch_size)\n",
        "    \n",
        "    loss=0\n",
        "\n",
        "    #Iterate over the entire training set for 10 times\n",
        "    for epoch in range(10):\n",
        "            \n",
        "        # Iterate over the number of mini-batches\n",
        "        for n_batch in range(num_batches-1):\n",
        "            \n",
        "            # Get the next mini-batches of samples for the training set\n",
        "            features, labels=get_next_batch(n_batch)\n",
        "              \n",
        "            # Perform the gradient descent step on that mini-batch and compute the loss value\n",
        "            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n",
        "            \n",
        "            loss+=loss_\n",
        "             \n",
        "        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss/num_batches)))\n",
        "        loss=0 \n",
        "    \n",
        "    \n",
        "    print('\\n\\nTesting the neural network:\\n')\n",
        "    # Compute the probability scores for the last mini-batch\n",
        "    prob_scores=sess.run(probs, feed_dict={x:features, y:labels})\n",
        "    \n",
        "    # Iterate over the features and labels from the last mini-batch as well as\n",
        "    # the predicitons made by the network, and compare them to check the performance\n",
        "    for f, l, p in zip(features, labels, prob_scores):\n",
        "    \n",
        "        # Get the class with the highest probability score\n",
        "        predicted_class=np.argmax(p)\n",
        "        # Get the actual probability score\n",
        "        predicted_class_score=np.max(p)\n",
        "     \n",
        "        print('Binary number: %s, decimal number: %i, predicted_class: %i, predicted_prob_score: %.3f' \n",
        "              %(str(f), l, predicted_class, predicted_class_score))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Generation of Data...\n",
            "\n",
            "binary number: 0000, decimal number: 0\n",
            "binary number: 0001, decimal number: 1\n",
            "binary number: 0010, decimal number: 2\n",
            "binary number: 0011, decimal number: 3\n",
            "binary number: 0100, decimal number: 4\n",
            "binary number: 0101, decimal number: 5\n",
            "binary number: 0110, decimal number: 6\n",
            "binary number: 0111, decimal number: 7\n",
            "binary number: 1000, decimal number: 8\n",
            "binary number: 1001, decimal number: 9\n",
            "\n",
            "\n",
            "Mini-batch of features: \n",
            "\n",
            "[[1. 0. 0. 1.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 1. 1. 1.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 1. 1.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 1. 1. 0.]]\n",
            "\n",
            "\n",
            "Mini-batch of labels: \n",
            "\n",
            "[9 0 7 1 8 3 2 6]\n",
            "\n",
            "\n",
            "Start of the training...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch_nr.: 0, loss: 2.169\n",
            "epoch_nr.: 1, loss: 1.572\n",
            "epoch_nr.: 2, loss: 1.182\n",
            "epoch_nr.: 3, loss: 0.887\n",
            "epoch_nr.: 4, loss: 0.665\n",
            "epoch_nr.: 5, loss: 0.503\n",
            "epoch_nr.: 6, loss: 0.387\n",
            "epoch_nr.: 7, loss: 0.305\n",
            "epoch_nr.: 8, loss: 0.247\n",
            "epoch_nr.: 9, loss: 0.205\n",
            "\n",
            "\n",
            "Testing the neural network:\n",
            "\n",
            "Binary number: [0. 0. 1. 0.], decimal number: 2, predicted_class: 2, predicted_prob_score: 0.832\n",
            "Binary number: [0. 1. 1. 0.], decimal number: 6, predicted_class: 6, predicted_prob_score: 0.787\n",
            "Binary number: [0. 1. 0. 0.], decimal number: 4, predicted_class: 4, predicted_prob_score: 0.862\n",
            "Binary number: [0. 1. 0. 1.], decimal number: 5, predicted_class: 5, predicted_prob_score: 0.824\n",
            "Binary number: [1. 0. 0. 1.], decimal number: 9, predicted_class: 9, predicted_prob_score: 0.870\n",
            "Binary number: [0. 0. 0. 0.], decimal number: 0, predicted_class: 0, predicted_prob_score: 0.823\n",
            "Binary number: [0. 1. 1. 1.], decimal number: 7, predicted_class: 7, predicted_prob_score: 0.793\n",
            "Binary number: [0. 0. 0. 1.], decimal number: 1, predicted_class: 1, predicted_prob_score: 0.785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbcnHuGtkPX8",
        "outputId": "d2b6118e-cb00-45b7-f202-ec30bd1f4b95"
      },
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "# list that will contain our training set\n",
        "training_data=[]\n",
        "\n",
        "# How many digits should this binary number have?\n",
        "n=11\n",
        "    \n",
        "#Mini-batch size\n",
        "batch_size=16\n",
        "\n",
        "print('\\n\\nGeneration of Data...\\n') \n",
        "for i in np.arange(2048):\n",
        "    \n",
        "    # Create a binary number of type string\n",
        "    b = bin(i)[2:]\n",
        "    l = len(b)\n",
        "    b = str(0) * (n - l) + b  \n",
        "\n",
        "    # Convert the binary string number to type float\n",
        "    features=np.array(list(b)).astype(float)\n",
        "    \n",
        "    # Create and normalize the corresponding decimal label\n",
        "    label=float(i)/2047\n",
        "    \n",
        "    # Put the feature-label instance into the list\n",
        "    training_data.append([features, label])\n",
        "    \n",
        "    if (i>=1 and i<11) or (i>=2038):\n",
        "        print('binary number: %s, decimal number: %d' %(b, i))\n",
        "        \n",
        "# shuffle the data\n",
        "shuffle(training_data)  \n",
        "\n",
        "# convert the list to np.array     \n",
        "training_data=np.array(training_data)\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "# Get the next mini-batch of training samples\n",
        "def get_next_batch(n_batch):\n",
        "    \n",
        "    # Get the next mini-batch of training samples from the dataset\n",
        "    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n",
        "    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n",
        "    \n",
        "    # Reshape the list of arrays into a nxn np.array\n",
        "    features = np.concatenate(features).reshape([batch_size,11])  \n",
        "    # Reshape the labels \n",
        "    labels=np.reshape(labels, [batch_size,1])\n",
        "    \n",
        "    return features, labels\n",
        "    \n",
        "features, labels=get_next_batch(n_batch=1)\n",
        "\n",
        "print(features)\n",
        "print(labels)\n",
        "#%%  \n",
        "\n",
        "# Create the training graph\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Define the placeholders for the features and the labels\n",
        "    x=tf.placeholder(dtype=tf.float32,shape=[batch_size,11], name='features')\n",
        "    y=tf.placeholder(dtype=tf.float32, shape=[batch_size,1], name='labels')\n",
        "           \n",
        "    # Create the weight matrices and the bias vectors \n",
        "    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n",
        "   \n",
        "    W1=tf.get_variable('W1',shape=[11,50], initializer=initializer)\n",
        "    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n",
        "    W3=tf.get_variable('W3',shape=[25,1], initializer=initializer)\n",
        "    \n",
        "    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n",
        "    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n",
        "\n",
        "    ### Define the forward propagation step with Dropout ###\n",
        "    \n",
        "    z1=tf.matmul(x,W1)+b1\n",
        "    a1=tf.nn.tanh(z1)\n",
        "    a1_dropout= tf.nn.dropout(a1, rate=0.25)\n",
        "    \n",
        "    z2=tf.matmul(a1_dropout,W2)+b2\n",
        "    a2=tf.nn.tanh(z2)\n",
        "    a2_dropout= tf.nn.dropout(a2, rate=0.25)\n",
        "    \n",
        "    # Outputlayer\n",
        "    predict_op=tf.nn.relu(tf.matmul(a2_dropout,W3))\n",
        "    \n",
        "    # Define the loss function\n",
        "    loss_op=tf.losses.mean_squared_error(y,predict_op)\n",
        "       \n",
        "    # Perform a gradient descent step\n",
        "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "    trainable_parameters = tf.trainable_variables()\n",
        "    gradients = tf.gradients(loss_op,trainable_parameters)\n",
        "    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n",
        "\n",
        "\n",
        "print('\\n\\nStart of the training...\\n')\n",
        "with tf.Session(graph=main_graph) as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # How many mini-batches in total?\n",
        "    num_batches=int(2048/batch_size)\n",
        "    \n",
        "    loss=0\n",
        "\n",
        "    #Iterate over the entire training set for 10 times\n",
        "    for epoch in range(10):\n",
        "            \n",
        "        # Iterate over the number of mini-batches\n",
        "        for n_batch in range(num_batches-1):\n",
        "            \n",
        "            # Get the next mini-batches of samples for the training set\n",
        "            features, labels=get_next_batch(n_batch)\n",
        "              \n",
        "            # Perform the gradient descent step on that mini-batch and compute the loss value\n",
        "            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n",
        "            loss+=loss_\n",
        "             \n",
        "        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss/num_batches)))\n",
        "        loss=0 \n",
        "\n",
        "    # Compute the prediction on the last mini-batch\n",
        "    prediction=sess.run(predict_op, feed_dict={x:features})\n",
        "    \n",
        "    # Iterate over the features and labels from the last mini-batch as well as\n",
        "    # the predicitons made by the network, and compare them to check the performance\n",
        "    for f, l, p in zip(features, labels, prediction):\n",
        "        \n",
        "        # Rescale the predictions and labels back into their original value range\n",
        "        p=p*2047\n",
        "        l=l*2047\n",
        "        \n",
        "        print('Binary number: %s,  label: %i, prediciton %i' %(str(f), l,p))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Generation of Data...\n",
            "\n",
            "binary number: 00000000001, decimal number: 1\n",
            "binary number: 00000000010, decimal number: 2\n",
            "binary number: 00000000011, decimal number: 3\n",
            "binary number: 00000000100, decimal number: 4\n",
            "binary number: 00000000101, decimal number: 5\n",
            "binary number: 00000000110, decimal number: 6\n",
            "binary number: 00000000111, decimal number: 7\n",
            "binary number: 00000001000, decimal number: 8\n",
            "binary number: 00000001001, decimal number: 9\n",
            "binary number: 00000001010, decimal number: 10\n",
            "binary number: 11111110110, decimal number: 2038\n",
            "binary number: 11111110111, decimal number: 2039\n",
            "binary number: 11111111000, decimal number: 2040\n",
            "binary number: 11111111001, decimal number: 2041\n",
            "binary number: 11111111010, decimal number: 2042\n",
            "binary number: 11111111011, decimal number: 2043\n",
            "binary number: 11111111100, decimal number: 2044\n",
            "binary number: 11111111101, decimal number: 2045\n",
            "binary number: 11111111110, decimal number: 2046\n",
            "binary number: 11111111111, decimal number: 2047\n",
            "[[0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1.]\n",
            " [1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1.]\n",
            " [1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1.]\n",
            " [1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0.]\n",
            " [1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0.]\n",
            " [0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.]\n",
            " [1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1.]\n",
            " [1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1.]]\n",
            "[[0.21836834391792867]\n",
            " [0.06790425012212994]\n",
            " [0.7743038593063019]\n",
            " [0.8466047874938935]\n",
            " [0.6756228627259404]\n",
            " [0.5310210063507572]\n",
            " [0.03517342452369321]\n",
            " [0.9697117733268197]\n",
            " [0.3444064484611627]\n",
            " [0.015632633121641426]\n",
            " [0.5852467024914509]\n",
            " [0.5530043966780654]\n",
            " [0.5266243282852956]\n",
            " [0.3512457254518808]\n",
            " [0.659990229604299]\n",
            " [0.6648754274548119]]\n",
            "\n",
            "\n",
            "Start of the training...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch_nr.: 0, loss: 0.284\n",
            "epoch_nr.: 1, loss: 0.178\n",
            "epoch_nr.: 2, loss: 0.118\n",
            "epoch_nr.: 3, loss: 0.073\n",
            "epoch_nr.: 4, loss: 0.055\n",
            "epoch_nr.: 5, loss: 0.044\n",
            "epoch_nr.: 6, loss: 0.037\n",
            "epoch_nr.: 7, loss: 0.034\n",
            "epoch_nr.: 8, loss: 0.033\n",
            "epoch_nr.: 9, loss: 0.031\n",
            "Binary number: [0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0.],  label: 974, prediciton 553\n",
            "Binary number: [0. 0. 1. 1. 1. 0. 1. 0. 1. 0. 0.],  label: 468, prediciton 313\n",
            "Binary number: [1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1.],  label: 1283, prediciton 687\n",
            "Binary number: [0. 0. 1. 1. 0. 1. 0. 1. 1. 1. 0.],  label: 430, prediciton 432\n",
            "Binary number: [1. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1.],  label: 1337, prediciton 1455\n",
            "Binary number: [0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 1.],  label: 445, prediciton 73\n",
            "Binary number: [1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1.],  label: 1785, prediciton 948\n",
            "Binary number: [0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.],  label: 288, prediciton 0\n",
            "Binary number: [1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0.],  label: 1266, prediciton 1691\n",
            "Binary number: [0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1.],  label: 549, prediciton 1356\n",
            "Binary number: [0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0.],  label: 602, prediciton 933\n",
            "Binary number: [1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1.],  label: 1833, prediciton 2257\n",
            "Binary number: [1. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.],  label: 1063, prediciton 1499\n",
            "Binary number: [1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.],  label: 1487, prediciton 1465\n",
            "Binary number: [1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.],  label: 1264, prediciton 1289\n",
            "Binary number: [1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0.],  label: 1500, prediciton 1094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7cMhxrPkYG1",
        "outputId": "f5cd573e-e137-40c0-99c2-43c2146bf5bc"
      },
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "# list that will contain our training set\n",
        "training_data=[]\n",
        "\n",
        "# How many digits should this binary number have?\n",
        "n=11\n",
        "    \n",
        "#Mini-batch size\n",
        "batch_size=16\n",
        "\n",
        "print('\\n\\nGeneration of Data...\\n') \n",
        "for i in np.arange(2048):\n",
        "    \n",
        "    # Create a binary number of type string\n",
        "    b = bin(i)[2:]\n",
        "    l = len(b)\n",
        "    b = str(0) * (n - l) + b  \n",
        "\n",
        "    # Convert the binary string number to type float\n",
        "    features=np.array(list(b)).astype(float)\n",
        "    \n",
        "    # Create and normalize the corresponding decimal label\n",
        "    label=float(i)/2047\n",
        "    \n",
        "    # Put the feature-label instance into the list\n",
        "    training_data.append([features, label])\n",
        "    \n",
        "    if (i>=1 and i<11) or (i>=2038):\n",
        "        print('binary number: %s, decimal number: %d' %(b, i))\n",
        "        \n",
        "# shuffle the data\n",
        "shuffle(training_data)  \n",
        "\n",
        "# convert the list to np.array     \n",
        "training_data=np.array(training_data)\n",
        "\n",
        "\n",
        "#%%\n",
        "\n",
        "# Get the next mini-batch of training samples\n",
        "def get_next_batch(n_batch):\n",
        "    \n",
        "    # Get the next mini-batch of training samples from the dataset\n",
        "    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n",
        "    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n",
        "    \n",
        "    # Reshape the list of arrays into a nxn np.array\n",
        "    features = np.concatenate(features).reshape([batch_size,11])  \n",
        "    # Reshape the labels \n",
        "    labels=np.reshape(labels, [batch_size,1])\n",
        "    \n",
        "    return features, labels\n",
        "    \n",
        "features, labels=get_next_batch(n_batch=1)\n",
        "\n",
        "print(features)\n",
        "print(labels)\n",
        "#%%  \n",
        "\n",
        "# Create the training graph\n",
        "main_graph=tf.Graph()\n",
        "\n",
        "with main_graph.as_default():\n",
        "    \n",
        "    # Define the placeholders for the features and the labels\n",
        "    x=tf.placeholder(dtype=tf.float32,shape=[batch_size,11], name='features')\n",
        "    y=tf.placeholder(dtype=tf.float32, shape=[batch_size,1], name='labels')\n",
        "           \n",
        "    # Create the weight matrices and the bias vectors \n",
        "    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n",
        "   \n",
        "    W1=tf.get_variable('W1',shape=[11,50], initializer=initializer)\n",
        "    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n",
        "    W3=tf.get_variable('W3',shape=[25,1], initializer=initializer)\n",
        "    \n",
        "    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n",
        "    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n",
        "\n",
        "    ### Define the forward propagation step ###\n",
        "    \n",
        "    # First hidden layer\n",
        "    z1=tf.matmul(x,W1)+b1\n",
        "    a1=tf.nn.tanh(z1)\n",
        "    \n",
        "    # Second hidden layer\n",
        "    z2=tf.matmul(a1,W2)+b2\n",
        "    a2=tf.nn.tanh(z2)\n",
        "    \n",
        "    # Outputlayer\n",
        "    predict_op=tf.nn.relu(tf.matmul(a2,W3))\n",
        "    \n",
        "\t\n",
        "\t\n",
        "\t#### L2-Regularization ####\n",
        "\t\n",
        "\t# Collect the weight matrices\n",
        "    weight_matrices=[var for var in tf.trainable_variables() if 'bias' not in var.name] \n",
        "\t#Apply the L2 regularization to the weight matrices\n",
        "    l2_losses=[tf.nn.l2_loss(w) for w in weight_matrices]\n",
        "    l2_loss = tf.add_n(l2_losses)\n",
        "    \n",
        "\t# Regularization rate\n",
        "    alpha=0.1\n",
        "    \n",
        "\t# Add the L2 regularization to the regular loss function\n",
        "    loss_op=tf.losses.mean_squared_error(y,predict_op)+alpha*l2_loss\n",
        "\t\n",
        "\t########\n",
        "       \n",
        "    # Perform a gradient descent step\n",
        "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "    trainable_parameters = tf.trainable_variables()\n",
        "    gradients = tf.gradients(loss_op,trainable_parameters)\n",
        "    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n",
        "\n",
        "\n",
        "print('\\n\\nStart of the training...\\n')\n",
        "with tf.Session(graph=main_graph) as sess:\n",
        "    \n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # How many mini-batches in total?\n",
        "    num_batches=int(2048/batch_size)\n",
        "    \n",
        "    loss=0\n",
        "\n",
        "    #Iterate over the entire training set for 10 times\n",
        "    for epoch in range(10):\n",
        "            \n",
        "        # Iterate over the number of mini-batches\n",
        "        for n_batch in range(num_batches-1):\n",
        "            \n",
        "            # Get the next mini-batches of samples for the training set\n",
        "            features, labels=get_next_batch(n_batch)\n",
        "              \n",
        "            # Perform the gradient descent step on that mini-batch and compute the loss value\n",
        "            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n",
        "            loss+=loss_\n",
        "             \n",
        "        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss/num_batches)))\n",
        "        loss=0 \n",
        "\n",
        "    # Compute the prediction on the last mini-batch\n",
        "    prediction=sess.run(predict_op, feed_dict={x:features})\n",
        "    \n",
        "    # Iterate over the features and labels from the last mini-batch as well as\n",
        "    # the predicitons made by the network, and compare them to check the performance\n",
        "    for f, l, p in zip(features, labels, prediction):\n",
        "        \n",
        "        # Rescale the predictions and labels back into their original value range\n",
        "        p=p*2047\n",
        "        l=l*2047\n",
        "        \n",
        "        print('Binary number: %s,  label: %i, prediciton %i' %(str(f), l,p))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Generation of Data...\n",
            "\n",
            "binary number: 00000000001, decimal number: 1\n",
            "binary number: 00000000010, decimal number: 2\n",
            "binary number: 00000000011, decimal number: 3\n",
            "binary number: 00000000100, decimal number: 4\n",
            "binary number: 00000000101, decimal number: 5\n",
            "binary number: 00000000110, decimal number: 6\n",
            "binary number: 00000000111, decimal number: 7\n",
            "binary number: 00000001000, decimal number: 8\n",
            "binary number: 00000001001, decimal number: 9\n",
            "binary number: 00000001010, decimal number: 10\n",
            "binary number: 11111110110, decimal number: 2038\n",
            "binary number: 11111110111, decimal number: 2039\n",
            "binary number: 11111111000, decimal number: 2040\n",
            "binary number: 11111111001, decimal number: 2041\n",
            "binary number: 11111111010, decimal number: 2042\n",
            "binary number: 11111111011, decimal number: 2043\n",
            "binary number: 11111111100, decimal number: 2044\n",
            "binary number: 11111111101, decimal number: 2045\n",
            "binary number: 11111111110, decimal number: 2046\n",
            "binary number: 11111111111, decimal number: 2047\n",
            "[[1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1.]\n",
            " [1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
            " [0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
            " [0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0.]\n",
            " [1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1.]\n",
            " [1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0.]\n",
            " [1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1.]\n",
            " [1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0.]\n",
            " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1.]\n",
            " [1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1.]\n",
            " [0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0.]]\n",
            "[[0.7860283341475329]\n",
            " [0.7967757694186615]\n",
            " [0.2862725940400586]\n",
            " [0.21836834391792867]\n",
            " [0.2432828529555447]\n",
            " [0.7381533952125061]\n",
            " [0.6204201270151442]\n",
            " [0.7200781631656082]\n",
            " [0.625305324865657]\n",
            " [0.15486077186126038]\n",
            " [0.7962872496336102]\n",
            " [0.7503663898387885]\n",
            " [0.29653150952613583]\n",
            " [0.545676599902296]\n",
            " [0.15095261358085002]\n",
            " [0.16511968734733756]]\n",
            "\n",
            "\n",
            "Start of the training...\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch_nr.: 0, loss: 5.243\n",
            "epoch_nr.: 1, loss: 3.962\n",
            "epoch_nr.: 2, loss: 3.070\n",
            "epoch_nr.: 3, loss: 2.387\n",
            "epoch_nr.: 4, loss: 1.861\n",
            "epoch_nr.: 5, loss: 1.456\n",
            "epoch_nr.: 6, loss: 1.144\n",
            "epoch_nr.: 7, loss: 0.903\n",
            "epoch_nr.: 8, loss: 0.718\n",
            "epoch_nr.: 9, loss: 0.575\n",
            "Binary number: [0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.],  label: 991, prediciton 1032\n",
            "Binary number: [1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0.],  label: 1816, prediciton 1507\n",
            "Binary number: [1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1.],  label: 2009, prediciton 1601\n",
            "Binary number: [1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1.],  label: 1635, prediciton 1383\n",
            "Binary number: [1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.],  label: 1982, prediciton 1669\n",
            "Binary number: [0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1.],  label: 905, prediciton 947\n",
            "Binary number: [1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1.],  label: 1269, prediciton 1145\n",
            "Binary number: [0. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0.],  label: 622, prediciton 788\n",
            "Binary number: [1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1.],  label: 1147, prediciton 1087\n",
            "Binary number: [0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1.],  label: 541, prediciton 726\n",
            "Binary number: [0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.],  label: 164, prediciton 412\n",
            "Binary number: [1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0.],  label: 1590, prediciton 1385\n",
            "Binary number: [1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1.],  label: 1109, prediciton 1005\n",
            "Binary number: [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1.],  label: 1203, prediciton 1110\n",
            "Binary number: [0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0.],  label: 220, prediciton 447\n",
            "Binary number: [1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1.],  label: 1683, prediciton 1407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyeTN-9Nkm0D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}